"""
Analyze TopoSleuth experimental results
"""
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import os

class ResultsAnalyzer:
    """Analyze experimental results and generate insights"""
    
    def __init__(self, data_dir="data"):
        self.data_dir = data_dir
        self.events = None
        self.detections = None
        self.performance = None
        
    def load_data(self):
        """Load all data files"""
        print("Loading data...")
        
        self.events = pd.read_csv(f"{self.data_dir}/lldp_events.csv")
        self.detections = pd.read_csv(f"{self.data_dir}/detections.csv")
        self.performance = pd.read_csv(f"{self.data_dir}/performance_metrics.csv")
        
        # Convert timestamps
        self.events['timestamp'] = pd.to_datetime(self.events['timestamp'])
        self.detections['timestamp'] = pd.to_datetime(self.detections['timestamp'])
        self.performance['timestamp'] = pd.to_datetime(self.performance['timestamp'])
        
        print(f"Loaded {len(self.events):,} events, {len(self.detections):,} detections")
    
    def analyze_detection_performance(self):
        """Analyze detection performance by attack type"""
        print("\n" + "=" * 60)
        print("DETECTION PERFORMANCE ANALYSIS")
        print("=" * 60)
        
        # Join events and detections
        attack_events = self.events[self.events['attack_type'].notna()]
        
        # Calculate detection rates by attack type
        detection_stats = []
        for attack_type in attack_events['attack_type'].unique():
            type_events = attack_events[attack_events['attack_type'] == attack_type]
            detected = type_events[type_events['detected'] == 'YES']
            
            total = len(type_events)
            detected_count = len(detected)
            detection_rate = (detected_count / total) * 100 if total > 0 else 0
            
            # Get average latency for this attack type
            type_detections = self.detections[self.detections['attack_type'] == attack_type]
            avg_latency = type_detections['latency_ms'].mean() if len(type_detections) > 0 else 0
            
            detection_stats.append({
                'attack_type': attack_type,
                'total_attacks': total,
                'detected': detected_count,
                'detection_rate': detection_rate,
                'avg_latency_ms': avg_latency,
                'decoy_hits': len(type_events[type_events['decoy_link_hit'] == 'YES'])
            })
        
        stats_df = pd.DataFrame(detection_stats)
        stats_df = stats_df.sort_values('detection_rate', ascending=False)
        
        print("\nDetection Performance by Attack Type:")
        print("-" * 60)
        for _, row in stats_df.iterrows():
            print(f"{row['attack_type']:25s}: {row['detection_rate']:5.1f}% "
                  f"({row['detected']:4d}/{row['total_attacks']:4d}) | "
                  f"Latency: {row['avg_latency_ms']:5.1f}ms | "
                  f"Decoy hits: {row['decoy_hits']:3d}")
        
        return stats_df
    
    def analyze_decoy_effectiveness(self):
        """Analyze decoy link effectiveness"""
        print("\n" + "=" * 60)
        print("DECOY LINK EFFECTIVENESS ANALYSIS")
        print("=" * 60)
        
        # Decoy-based detections
        decoy_detections = self.detections[self.detections['decoy_link_hit'] == 'YES']
        behavioral_detections = self.detections[self.detections['decoy_link_hit'] == 'NO']
        
        print(f"\nDecoy-based detections: {len(decoy_detections):,}")
        print(f"Behavioral detections: {len(behavioral_detections):,}")
        
        if len(decoy_detections) > 0 and len(behavioral_detections) > 0:
            # Compare metrics
            decoy_latency = decoy_detections['latency_ms'].mean()
            behavioral_latency = behavioral_detections['latency_ms'].mean()
            latency_improvement = ((behavioral_latency - decoy_latency) / behavioral_latency) * 100
            
            decoy_confidence = decoy_detections['dc_confidence'].mean()
            behavioral_confidence = behavioral_detections['dc_confidence'].mean()
            confidence_improvement = ((decoy_confidence - behavioral_confidence) / behavioral_confidence) * 100
            
            print(f"\nPerformance Comparison:")
            print(f"  Average latency:")
            print(f"    Decoy-based: {decoy_latency:.1f}ms")
            print(f"    Behavioral:  {behavioral_latency:.1f}ms")
            print(f"    Improvement: {latency_improvement:.1f}% faster")
            
            print(f"\n  Confidence scores:")
            print(f"    Decoy-based: {decoy_confidence:.3f}")
            print(f"    Behavioral:  {behavioral_confidence:.3f}")
            print(f"    Improvement: {confidence_improvement:.1f}% higher")
        
        # Which attacks were caught by decoys
        if len(decoy_detections) > 0:
            attack_counts = decoy_detections['attack_type'].value_counts()
            print(f"\nAttacks caught by decoy links:")
            for attack_type, count in attack_counts.items():
                percentage = count / len(decoy_detections) * 100
                print(f"  {attack_type:25s}: {count:4d} ({percentage:5.1f}%)")
    
    def analyze_performance_metrics(self):
        """Analyze system performance metrics"""
        print("\n" + "=" * 60)
        print("SYSTEM PERFORMANCE ANALYSIS")
        print("=" * 60)
        
        # Filter to attack periods
        attack_periods = self.performance[self.performance['measurement_type'] == 'under_attack']
        
        if len(attack_periods) > 0:
            print(f"\nPerformance under attack (averaged over {len(attack_periods)} runs):")
            print("-" * 60)
            
            metrics = ['detection_latency_mean', 'false_positive_rate', 
                      'cpu_overhead_percent', 'memory_usage_mb', 'network_overhead_kbps']
            
            for metric in metrics:
                if metric in attack_periods.columns:
                    # Convert to numeric
                    values = pd.to_numeric(attack_periods[metric], errors='coerce')
                    mean_val = values.mean()
                    std_val = values.std()
                    
                    metric_name = metric.replace('_', ' ').title()
                    print(f"  {metric_name:25s}: {mean_val:6.1f} ¬± {std_val:4.1f}")
        
        # Compare normal vs attack
        normal = self.performance[self.performance['measurement_type'] == 'normal_operation']
        attack = self.performance[self.performance['measurement_type'] == 'under_attack']
        
        if len(normal) > 0 and len(attack) > 0:
            normal_cpu = pd.to_numeric(normal['cpu_overhead_percent'], errors='coerce').mean()
            attack_cpu = pd.to_numeric(attack['cpu_overhead_percent'], errors='coerce').mean()
            cpu_increase = ((attack_cpu - normal_cpu) / normal_cpu) * 100
            
            print(f"\nCPU Overhead Increase under attack: {cpu_increase:.1f}%")
            print(f"  Normal: {normal_cpu:.1f}%")
            print(f"  Under attack: {attack_cpu:.1f}%")
    
    def generate_summary_report(self):
        """Generate comprehensive summary report"""
        print("\n" + "=" * 60)
        print("EXPERIMENTAL RESULTS SUMMARY")
        print("=" * 60)
        
        # Load data if not already loaded
        if self.events is None:
            self.load_data()
        
        # Calculate key metrics
        total_events = len(self.events)
        attack_events = self.events['attack_type'].notna().sum()
        normal_events = self.events['attack_type'].isna().sum()
        
        attack_detected = self.events[self.events['detected'] == 'YES']['attack_type'].notna().sum()
        overall_detection_rate = (attack_detected / attack_events) * 100 if attack_events > 0 else 0
        
        decoy_hits = self.events['decoy_link_hit'].value_counts().get('YES', 0)
        decoy_percentage = (decoy_hits / attack_events) * 100 if attack_events > 0 else 0
        
        print(f"\nüìä Dataset Overview:")
        print(f"  Total events: {total_events:,}")
        print(f"  Attack events: {attack_events:,}")
        print(f"  Normal events: {normal_events:,}")
        print(f"  Detection records: {len(self.detections):,}")
        
        print(f"\nüéØ Key Performance Metrics:")
        print(f"  Overall detection rate: {overall_detection_rate:.1f}%")
        print(f"  Decoy link hits: {decoy_hits:,} ({decoy_percentage:.1f}% of attacks)")
        
        if len(self.detections) > 0:
            avg_latency = pd.to_numeric(self.detections['latency_ms'], errors='coerce').mean()
            print(f"  Average detection latency: {avg_latency:.1f}ms")
        
        # Performance metrics
        if self.performance is not None:
            attack_perf = self.performance[self.performance['measurement_type'] == 'under_attack']
            if len(attack_perf) > 0:
                cpu_avg = pd.to_numeric(attack_perf['cpu_overhead_percent'], errors='coerce').mean()
                memory_avg = pd.to_numeric(attack_perf['memory_usage_mb'], errors='coerce').mean()
                print(f"  Average CPU overhead (attack): {cpu_avg:.1f}%")
                print(f"  Average memory usage (attack): {memory_avg:.1f} MB")
        
        print(f"\n‚úÖ Analysis complete.")


if __name__ == "__main__":
    # Create output directory
    os.makedirs("analysis/output", exist_ok=True)
    
    # Run analysis
    analyzer = ResultsAnalyzer()
    analyzer.load_data()
    
    # Run analyses
    detection_stats = analyzer.analyze_detection_performance()
    analyzer.analyze_decoy_effectiveness()
    analyzer.analyze_performance_metrics()
    
    # Generate final summary
    analyzer.generate_summary_report()
    
    # Save detection stats
    detection_stats.to_csv("analysis/output/detection_stats.csv", index=False)
    print(f"\nüìÅ Results saved to 'analysis/output/'")